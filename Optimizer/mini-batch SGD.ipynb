{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "\n",
    "# Optimizer classes  \n",
    "# Gradient Descent class\n",
    "class GD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "# Stochastic Gradient Descent class\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "# Mini-batch Stochastic Gradient Descent class\n",
    "class mini_SGD:\n",
    "    def __init__(self, lr=0.64):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Read MNIST Dataset\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size1 = 1\n",
    "batch_size2 = 64\n",
    "\n",
    "# Experiment setup\n",
    "optimizers = {\n",
    "    'SGD' : SGD(lr = 0.01),\n",
    "    'GD' : GD(lr = 0.01),\n",
    "    'mini_SGD' : mini_SGD(lr = 0.64)\n",
    "}\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "\n",
    "# Initialize networks & loss tracking  \n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784,\n",
    "        hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10\n",
    "    )\n",
    "    train_loss[key] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e480a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== epoch: 0 ==========\n",
      "SGD: 1.3661220968693282\n",
      "GD: 2.3184861723813155\n",
      "mini_SGD: 2.4405026959527785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     t_input \u001b[38;5;241m=\u001b[39m t_batch2            \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute gradients and update parameters\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mnetworks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizers[key]\u001b[38;5;241m.\u001b[39mupdate(networks[key]\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Compute and record loss\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master\\common\\multi_layer_net.py:152\u001b[0m, in \u001b[0;36mMultiLayerNet.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    150\u001b[0m layers\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m--> 152\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[0;32m    155\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mC:\\Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master\\common\\layers.py:62\u001b[0m, in \u001b[0;36mAffine.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout):\n\u001b[1;32m---> 62\u001b[0m     dx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdW \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mT, dout)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dout, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# Training loop\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Sample a data for SGD\n",
    "    batch_mask1 = np.random.choice(train_size, batch_size1)\n",
    "    x_batch1 = x_train[batch_mask1]\n",
    "    t_batch1 = t_train[batch_mask1]\n",
    "\n",
    "    # Sample a mini-batch for SGD\n",
    "    batch_mask2 = np.random.choice(train_size, batch_size2)\n",
    "    x_batch2 = x_train[batch_mask2]\n",
    "    t_batch2 = t_train[batch_mask2]\n",
    "\n",
    "\n",
    "    for key in optimizers.keys():\n",
    "        # Use full dataset for GD, mini-batch for SGD\n",
    "        if key == 'GD':\n",
    "            x_input = x_train\n",
    "            t_input = t_train\n",
    "        elif key == 'SGD':\n",
    "            x_input = x_batch1\n",
    "            t_input = t_batch1\n",
    "        else:\n",
    "            x_input = x_batch2\n",
    "            t_input = t_batch2            \n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        grads = networks[key].gradient(x_input, t_input)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "\n",
    "        # Compute and record loss\n",
    "        loss = networks[key].loss(x_input, t_input)\n",
    "        train_loss[key].append(loss)\n",
    "\n",
    "    # Print loss every 100 iterations\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"========== epoch: \" + str(epoch) + \" ==========\")\n",
    "        for key in optimizers.keys():\n",
    "            if key == 'GD':\n",
    "                x_input = x_train\n",
    "                t_input = t_train\n",
    "            elif key == 'SGD':\n",
    "                x_input = x_batch1\n",
    "                t_input = t_batch1\n",
    "            else:  # e.g., 'mini_SGD' or others\n",
    "                x_input = x_batch2\n",
    "                t_input = t_batch2\n",
    "                \n",
    "            loss = networks[key].loss(x_input, t_input)\n",
    "            print(f\"{key}: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
